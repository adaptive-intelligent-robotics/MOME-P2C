hydra:
    run:
        dir: ./output/${env_name}/${algo_name}/${now:%Y-%m-%d_%H%M%S}_${seed}


#Env config
seed: 42
env_name: ${env.name}
algo_name: ${algo.name}
fixed_init_state: True
episode_length: 1000


# MOO parameters
pareto_front_max_length: 50

# Initialisation parameters
num_evaluations: 1024000
num_init_cvt_samples: 50000 
num_centroids: 128 

# Policy parameters 
policy_hidden_layer_sizes: [64, 64] 
reward_offset: 0.

# Defaults
defaults:
    - algo: mome-pgx
    - env: walker2d_multi

# Emitter parameters
iso_sigma: 0.005 
line_sigma: 0.05 
total_batch_size: 256

# TD3 params
replay_buffer_size: 1000000 
critic_hidden_layer_size: [256, 256] 
critic_learning_rate: 0.0003
greedy_learning_rate: 0.0003
policy_learning_rate: 0.005
noise_clip: 0.5 
policy_noise: 0.2 
discount: 0.99 
transitions_batch_size: 256 
soft_tau_update: 0.005 
policy_delay: 2
num_critic_training_steps: 3000 
num_pg_training_steps: 150 

# Logging parameters
metrics_log_period: 40
plot_repertoire_period: 400
checkpoint_period: 400
num_save_visualisations: 5

# Common metrics to log
wandb_metrics_keys: [
    "moqd_score",
    "qd_sparsity_score",
    "max_hypervolume",
    "max_sum_scores",
    "coverage",
    "global_hypervolume",
    "global_sparsity",
    "total_num_solutions",
    "running_reward_count",
]
